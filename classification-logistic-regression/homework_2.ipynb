{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will be using a banking marketing dataset. \n",
    "The dataset is associated with direct marketing campaigns of a banking institution. We want to find out the best strategies to improve the next marketing campaign. How can the bank have a greater effectiveness for future marketing campaigns? In order to answer this, we have to analyze the last marketing campaign the bank performed and identify the patterns that will help us find conclusions in order to develop future strategies.\n",
    "\n",
    "We have to predict whether a customer subscribes for term deposit or not using the following attributes: \n",
    "1 - age (numeric)<br>\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')<br>\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')<br>\n",
    "6 - balance: balance amount (numeric)<br>\n",
    "7 - housing: has housing loan? (categorical: 'no','yes','unknown')<br>\n",
    "8 - loan: has personal loan? (categorical: 'no','yes','unknown')<br>\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')<br>\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)<br>\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)<br>\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')<br>\n",
    "\n",
    "features_ex2.xlsx contains the features. It has 4521 records. First 3165 observations are used for training dataset, next 678 observations are used for cross validation dataset and final 678 observations are used for test dataset.\n",
    "\n",
    "label_ex2.xlsx contains the label: \"yes\" or \"no\". First 3165 observations are used for training dataset, next 678 observations are used for cross validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1787</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>oct</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>4789</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>may</td>\n",
       "      <td>1</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1350</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>16</td>\n",
       "      <td>apr</td>\n",
       "      <td>1</td>\n",
       "      <td>330</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1476</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>3</td>\n",
       "      <td>jun</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          job  marital  education default  balance housing loan  \\\n",
       "0   30   unemployed  married    primary      no     1787      no   no   \n",
       "1   33     services  married  secondary      no     4789     yes  yes   \n",
       "2   35   management   single   tertiary      no     1350     yes   no   \n",
       "3   30   management  married   tertiary      no     1476     yes  yes   \n",
       "4   59  blue-collar  married  secondary      no        0     yes   no   \n",
       "\n",
       "    contact  day month  campaign  pdays  previous poutcome  \n",
       "0  cellular   19   oct         1     -1         0  unknown  \n",
       "1  cellular   11   may         1    339         4  failure  \n",
       "2  cellular   16   apr         1    330         1  failure  \n",
       "3   unknown    3   jun         4     -1         0  unknown  \n",
       "4   unknown    5   may         1     -1         0  unknown  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_excel(\"features_ex2.xlsx\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y\n",
       "0  no\n",
       "1  no\n",
       "2  no\n",
       "3  no\n",
       "4  no"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_excel(\"label_ex2.xlsx\")\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['job','marital','education','default','housing','loan','contact','month','poutcome']\n",
    "categorical = pd.get_dummies(X[categories])\n",
    "continuous = X.drop(columns=categories)\n",
    "X = pd.concat([continuous,categorical],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into train, cv and test set (70:15:15 ratio)\n",
    "X_train = X.iloc[0:3165,:]\n",
    "y_train = y.iloc[0:3165,:]\n",
    "X_cv = X.iloc[3165:3843,:]\n",
    "y_cv = y.iloc[3165:3843,:]\n",
    "X_test = X.iloc[3843:4521,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taufeeq\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4263: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  method=method,\n"
     ]
    }
   ],
   "source": [
    "#Changing Yes and No to 1 and 0\n",
    "mapping = {\"yes\":1, \"no\":0}\n",
    "y_cv.replace(mapping, inplace=True)\n",
    "y_train.replace(mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (3165, 50)\n",
      "y_train (3165, 1)\n",
      "X_cv (678, 50)\n",
      "y_cv (678, 1)\n",
      "X_test (678, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train \"+ str(X_train.shape))\n",
    "print(\"y_train \"+ str(y_train.shape))\n",
    "print(\"X_cv \"+ str(X_cv.shape))\n",
    "print(\"y_cv \"+ str(y_cv.shape))\n",
    "print(\"X_test \"+ str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "As discussed in previous exercise, standardization is important when a number of features with different scales are involed. \n",
    "\n",
    "Q. Use StandardScaler from sklearn.preprocessing to standardize the continuous features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taufeeq\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "continuous_variables = ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "X_train[continuous_variables] = pd.DataFrame(scaler.fit_transform(X_train[continuous_variables]), columns = ['age', 'balance', 'day', 'campaign', 'pdays', 'previous'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv = X_cv.reset_index()\n",
    "del X_cv['index']\n",
    "\n",
    "X_test = X_test.reset_index()\n",
    "del X_test['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarily use the above list to replace the continuous columns in X_cv and X_test to scaled columns. Use transform method.\n",
    "X_cv[continuous_variables] = pd.DataFrame(scaler.transform(X_cv[continuous_variables]))\n",
    "X_test[continuous_variables] = pd.DataFrame(scaler.transform(X_test[continuous_variables]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "As previously mentioned, the scikit-learn classification API makes it easy to train a classifier. \n",
    "\n",
    "\n",
    "Q. Use LogisticRegression from sklearn.linear_model to make a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the classifier with default parameters and fitting the classifier on training data and labels\n",
    "\n",
    "logreg = LogisticRegression(solver = 'liblinear')\n",
    "logreg.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the output for cross validation dataset\n",
    "y_new = logreg.predict(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8908554572271387\n",
      "Precision:  0.45\n",
      "Recall:  0.125\n"
     ]
    }
   ],
   "source": [
    "#Implementation of accuracy, precision, recall\n",
    "from classification_utils import accuracy, precision, recall\n",
    "\n",
    "acc = accuracy(y_cv.values, y_new)\n",
    "print('Accuracy:', acc)\n",
    "\n",
    "pre = precision(y_cv.values, y_new)\n",
    "print('Precision: ', pre)\n",
    "\n",
    "rec = recall(y_cv.values, y_new)\n",
    "print('Recall: ', rec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the measure of how well our model predicts future outcomes. But accuracy alone isn't always sufficient to differentiate one model from another. That is when we use precision and recall. When the costs of False Positive is high, Precision is a good measure whereas when the cost of False Negative is high, Recall is a good measure to select our best model. There are scenarios where precision is better than accuracy. For example, in email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model.<br/>\n",
    "\n",
    "We should use precision in this case. The prediction focuses on whether a customer subscribes for term deposit or not. A false positive in this case means wasted expense if the user doesn't actually subscribe. Therefore, precision which emphasizes true positive is the measurement we should use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7778236156949029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_cv.values, logreg.predict_proba(X_cv)[:,1], pos_label =1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the ROC curve\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr)                    \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.plot([0, 1], [0, 1], 'y--')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('AUC:', roc_auc_score(y_cv.values,logreg.predict_proba(X_cv)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "\"Model tuning\" refers to model adjustments to better fit the data. This is separate from \"fitting\" or \"training\" the model. The fitting/training procedure is governed by the amount and quality of your training data, as the fitting algorithm is unique to each classifier (e.g. logistic regression or random forest). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: Build a model with hyperparameter 'C' set to 0.1 and penalty set to 'l1'. Make predictions on cross validation set and compute accuracy, precision and recall. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8982300884955752\n",
      "precision: 0.6\n",
      "Recall: 0.125\n"
     ]
    }
   ],
   "source": [
    "logreg1 = LogisticRegression(C = 0.1, penalty = 'l1', solver = 'liblinear')\n",
    "logreg1.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_new1 = logreg1.predict(X_cv)\n",
    "\n",
    "acc1 = accuracy(y_cv.values, y_new1)\n",
    "print('Accuracy:', acc1)\n",
    "\n",
    "pre1 = precision(y_cv.values, y_new1)\n",
    "print('precision:', pre1)\n",
    "\n",
    "rec1 = recall(y_cv.values, y_new1)\n",
    "print('Recall:', rec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: Build a model with hyperparameter 'C' set to 0.5 and penalty set to 'l1'. Make predictions on cross validation set and compute accuracy, precision and recall. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8938053097345132\n",
      "precision: 0.5\n",
      "Recall: 0.1388888888888889\n"
     ]
    }
   ],
   "source": [
    "logreg2 = LogisticRegression(C = 0.5, penalty = 'l1', solver = 'liblinear')\n",
    "logreg2.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_new2 = logreg2.predict(X_cv)\n",
    "\n",
    "acc2 = accuracy(y_cv.values, y_new2)\n",
    "print('Accuracy:', acc2)\n",
    "\n",
    "pre2 = precision(y_cv.values, y_new2)\n",
    "print('precision:', pre2)\n",
    "\n",
    "rec2 = recall(y_cv.values, y_new2)\n",
    "print('Recall:', rec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3: Build a model with hyperparameter 'C' set to 0.1 and penalty set to 'l2'. Make predictions on cross validation set and compute accuracy, precision and recall. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8982300884955752\n",
      "precision: 0.6\n",
      "Recall: 0.125\n"
     ]
    }
   ],
   "source": [
    "logreg3 = LogisticRegression(C = 0.1, penalty = 'l2', solver = 'liblinear')\n",
    "logreg3.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_new3 = logreg3.predict(X_cv)\n",
    "\n",
    "acc3 = accuracy(y_cv.values, y_new3)\n",
    "print('Accuracy:', acc3)\n",
    "\n",
    "pre3 = precision(y_cv.values, y_new3)\n",
    "print('precision:', pre3)\n",
    "\n",
    "rec3 = recall(y_cv.values, y_new3)\n",
    "print('Recall:', rec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 4: Build a model with hyperparameter 'C' set to 0.5 and penalty set to 'l2'. Make predictions on cross validation set and compute accuracy, precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8923303834808259\n",
      "precision: 0.47368421052631576\n",
      "Recall: 0.125\n"
     ]
    }
   ],
   "source": [
    "logreg4 = LogisticRegression(C = 0.5, penalty = 'l2', solver = 'liblinear')\n",
    "logreg4.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_new4 = logreg4.predict(X_cv)\n",
    "\n",
    "acc4 = accuracy(y_cv.values, y_new4)\n",
    "print('Accuracy:', acc4)\n",
    "\n",
    "pre4 = precision(y_cv.values, y_new4)\n",
    "print('precision:', pre4)\n",
    "\n",
    "rec4 = recall(y_cv.values, y_new4)\n",
    "print('Recall:', rec4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the third one, with hyperparameter 'C' set to 0.1 and penalty set to 'l2', is better since it has higher precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LogisticRegression(C = 0.1, penalty = 'l2', solver = 'liblinear')\n",
    "final_model.fit(X_train, np.ravel(y_train))\n",
    "predicted = pd.DataFrame(final_model.predict(X_test), columns = ['y'])\n",
    "\n",
    "back_map = {1:\"yes\", 0:\"no\"}\n",
    "predicted.replace(back_map, inplace=True)\n",
    "\n",
    "predicted.to_csv('result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
